{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vegaarellano\\Documents\\Gitlab\\instancesegmentation\\pytorch-img-seg\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import name\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import false\n",
    "from torch import nn, unsqueeze\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection.mask_rcnn import maskrcnn_resnet50_fpn\n",
    "import torchvision\n",
    "import nibabel as nib\n",
    "import napari\n",
    "import os\n",
    "#from vnet.vnet import VNet\n",
    "#from vnet.transformations import ResizeImage\n",
    "import random\n",
    "from datetime import date\n",
    "from collections import OrderedDict\n",
    "from monai.losses import FocalLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import autograd\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "#network with 3 conv-and-deconv steps used in paper\n",
    "\n",
    "class one_step_conv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(one_step_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # Level 1\n",
    "            nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, padding=1),            \n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Level 2            \n",
    "            nn.Conv2d(in_channels=out_ch, out_channels=out_ch, kernel_size=3, padding=1),            \n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.Dropout2d(p=0.2)             \n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "# class one_step_conv(nn.Module):\n",
    "#     def __init__(self, in_ch, out_ch):\n",
    "#         super(one_step_conv, self).__init__()\n",
    "#         self.conv = nn.Sequential(            \n",
    "#             nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm2d(out_ch),\n",
    "#             nn.Dropout(p = 0.1)\n",
    "            \n",
    "#         )\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         return self.conv(input)\n",
    "\n",
    "class UnetDeep3(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(UnetDeep3, self).__init__()\n",
    "        base_filter_num = 64\n",
    "        self.conv_down_1 = one_step_conv(in_ch, base_filter_num)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv_down_2 = one_step_conv(base_filter_num, base_filter_num*2)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.conv_down_3 = one_step_conv(base_filter_num*2, base_filter_num*4)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.conv_bottom = one_step_conv(base_filter_num*4, base_filter_num*8)\n",
    "        \n",
    "        \n",
    "        self.upsample_1 = nn.ConvTranspose2d(base_filter_num*8, base_filter_num*4, kernel_size=2, stride=2)\n",
    "        self.conv_up_1 = one_step_conv(base_filter_num*8, base_filter_num*4)\n",
    "        self.upsample_2 = nn.ConvTranspose2d(base_filter_num*4, base_filter_num*2, kernel_size=2, stride=2)\n",
    "        self.conv_up_2 = one_step_conv(base_filter_num*4, base_filter_num*2)\n",
    "        self.upsample_3 = nn.ConvTranspose2d(base_filter_num*2, base_filter_num, kernel_size=2, stride=2)\n",
    "        self.conv_up_3 = one_step_conv(base_filter_num*2, base_filter_num)\n",
    "        self.conv_out = nn.Conv2d(base_filter_num, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ###down########        \n",
    "        down_1 = self.conv_down_1(x)\n",
    "        pool_1 = self.pool1(down_1)\n",
    "        down_2 = self.conv_down_2(pool_1)\n",
    "        pool_2 = self.pool2(down_2)\n",
    "        down_3 = self.conv_down_3(pool_2)\n",
    "        pool_3 = self.pool3(down_3)\n",
    "        bottom = self.conv_bottom(pool_3)\n",
    "\n",
    "        up_1 = self.upsample_1(bottom)\n",
    "        merge1 = torch.cat([up_1, down_3], dim=1)\n",
    "        #print(merge1.shape)\n",
    "        up_1_out = self.conv_up_1(merge1)\n",
    "        up_2 = self.upsample_2(up_1_out)\n",
    "        merge2 = torch.cat([up_2, down_2], dim=1)\n",
    "        #print(merge2.shape)\n",
    "        up_2_out = self.conv_up_2(merge2)\n",
    "        up_3 = self.upsample_3(up_2_out)\n",
    "        merge3 = torch.cat([up_3, down_1], dim=1)\n",
    "        #print(merge3.shape)\n",
    "        up_3_out = self.conv_up_3(merge3)\n",
    "        end_out = self.conv_out(up_3_out)\n",
    "        #print(end_out.shape)\n",
    "        #return end_out\n",
    "        # Applying classification in the output layer\n",
    "        out = nn.Sigmoid()(end_out)\n",
    "        #out = nn.Softmax()(end_out)\n",
    "        #print(out.shape)\n",
    "        #out = nn.Sigmoid(end_out)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 224, 224])\n",
      "tensor(0.8233)\n",
      "tensor(0.0755)\n",
      "tensor([0.3213, 0.5990, 0.3876, 0.4177, 0.3085, 0.4112, 0.5947, 0.5021, 0.5149,\n",
      "        0.5263])\n",
      "tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 0.])\n",
      "tensor(0.6949)\n",
      "tensor(0.5189)\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "torch.manual_seed(200)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = torch.randn((1,1,224,224))\n",
    "    y = torch.randn((1,1,224,224)) # \n",
    "    y = (y > 0.5).float()\n",
    "\n",
    "    model = UnetDeep3(in_ch=1, out_ch=1)\n",
    "    prediction = model(x)\n",
    "    print(prediction.shape)\n",
    "    print(torch.max(prediction))\n",
    "    print(torch.min(prediction))\n",
    "    print(prediction[0,0,100,100:110])\n",
    "    print(y[0,0,100,100:110])\n",
    "    print(torch.sigmoid(torch.max(prediction)))\n",
    "    print(torch.sigmoid(torch.min(prediction)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2711390554904938"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Trying with Focal Loss\n",
    "loss = FocalLoss()\n",
    "loss_value = loss(prediction, y)\n",
    "loss_value.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03329070284962654"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Trying with WBCE\n",
    "loss = nn.BCELoss(weight=torch.FloatTensor([.05]))\n",
    "loss_value = loss(prediction, y)\n",
    "loss_value.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.1268,  1.3564,  0.5632,  ...,  1.8463,  0.9502, -0.3396],\n",
      "          [-0.6599, -0.9520, -0.4073,  ...,  1.6103, -1.6023, -1.3940],\n",
      "          [-0.2074,  0.8496, -1.0217,  ...,  0.1431, -0.3378,  0.8120],\n",
      "          ...,\n",
      "          [ 1.4933,  0.9621, -0.4328,  ...,  2.3392, -0.3927, -1.0324],\n",
      "          [-0.5816,  0.2777,  1.3490,  ...,  2.5849,  0.5224,  0.9784],\n",
      "          [ 1.2349, -0.5501,  0.4484,  ...,  0.9272,  0.5862, -1.0449]]]])\n",
      "tensor([[[[1., 0., 1.,  ..., 0., 1., 1.],\n",
      "          [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "          [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "          [1., 0., 1.,  ..., 1., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "# Example of loss functions\n",
    "torch.manual_seed(100)\n",
    "x = Variable(torch.randn(1,1,224,224))\n",
    "y = Variable(torch.FloatTensor(1,1,224,224).random_(2))\n",
    "print(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0500])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# double the loss for class 1 ( 3 classes in the loss)\n",
    "# Probability Background: 95% => Stroke: 5%\n",
    "# Weights inverted for BCE function\n",
    "class_weight = torch.FloatTensor([0.05])\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [2., 2., 2.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double the loss for last sample\n",
    "element_weight = torch.FloatTensor([1.0]*9 + [2.0]).view(-1, 1)\n",
    "print(element_weight)\n",
    "element_weight = element_weight.repeat(1, 3)\n",
    "element_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.6317, 1.5856, 0.4507,  ..., 1.9928, 0.3269, 0.8773],\n",
      "          [0.4167, 0.3264, 0.5101,  ..., 1.7925, 1.7858, 1.6156],\n",
      "          [0.5948, 1.2056, 0.3075,  ..., 0.6241, 0.5384, 1.1794],\n",
      "          ...,\n",
      "          [1.6959, 1.2857, 0.9328,  ..., 2.4312, 0.5160, 0.3046],\n",
      "          [1.0257, 0.8416, 0.2307,  ..., 0.0727, 0.9881, 1.2975],\n",
      "          [1.4902, 1.0056, 0.9423,  ..., 1.2606, 1.0286, 0.3014]]]])\n",
      "tensor([[[[0.0316, 0.0793, 0.0225,  ..., 0.0996, 0.0163, 0.0439],\n",
      "          [0.0208, 0.0163, 0.0255,  ..., 0.0896, 0.0893, 0.0808],\n",
      "          [0.0297, 0.0603, 0.0154,  ..., 0.0312, 0.0269, 0.0590],\n",
      "          ...,\n",
      "          [0.0848, 0.0643, 0.0466,  ..., 0.1216, 0.0258, 0.0152],\n",
      "          [0.0513, 0.0421, 0.0115,  ..., 0.0036, 0.0494, 0.0649],\n",
      "          [0.0745, 0.0503, 0.0471,  ..., 0.0630, 0.0514, 0.0151]]]])\n"
     ]
    }
   ],
   "source": [
    "bce_criterion = nn.BCEWithLogitsLoss(weight=None, reduce=False)\n",
    "bce_loss = bce_criterion(x, y)\n",
    "print(bce_loss)\n",
    "\n",
    "bce_criterion_class = nn.BCEWithLogitsLoss(weight=class_weight, reduce=False)\n",
    "bce_loss_class = bce_criterion_class(x, y)\n",
    "print(bce_loss_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "multi_criterion_class = nn.MultiLabelSoftMarginLoss(weight=class_weight, reduce=False)\n",
    "\n",
    "bce_criterion_element = nn.BCEWithLogitsLoss(weight=element_weight, reduce=False)\n",
    "multi_criterion_element = nn.MultiLabelSoftMarginLoss(weight=element_weight, reduce=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bce_loss_class = bce_criterion_class(x, y)\n",
    "\n",
    "\n",
    "bce_loss_element = bce_criterion_element(x, y)\n",
    "multi_loss_element = multi_criterion_element(x, y)\n",
    "\n",
    "print(bce_loss - multi_loss)\n",
    "print(bce_loss_class - multi_loss_class)\n",
    "print(bce_loss_element - multi_loss_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Samples: ============\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       dtype=torch.int32)\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1],\n",
      "       dtype=torch.int32)\n",
      "============= Precision recall: ============\n",
      "(tensor(0.4806), tensor(0.4790))\n",
      "(tensor(0.5712), tensor(0.5712))\n",
      "(tensor(0.5894), tensor(0.5712))\n",
      "============= F1 Score ============\n",
      "tensor(0.5712)\n",
      "============= F1 Score ============\n",
      "tensor(0.5712)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchmetrics.functional import precision_recall\n",
    "from torchmetrics import F1Score\n",
    "\n",
    "torch.manual_seed(200)\n",
    "preds  = torch.randn([1,1,25,25])\n",
    "target = torch.randn([1,1,25,25])\n",
    "target = (target > 0.5).int()\n",
    "preds = (preds > 0.5).int()\n",
    "preds = preds.reshape(-1)\n",
    "target = target.reshape(-1)\n",
    "\n",
    "\n",
    "print(\"============= Samples: ============\")\n",
    "print(target[0:20])\n",
    "print(preds[0:20])\n",
    "print(\"============= Precision recall: ============\")\n",
    "result = precision_recall(preds, target, average='macro', num_classes=2)\n",
    "print(result)\n",
    "result = precision_recall(preds, target, average='micro')\n",
    "print(result)\n",
    "result = precision_recall(preds, target, average='weighted', num_classes=2)\n",
    "print(result)\n",
    "\n",
    "print(\"============= F1 Score ============\")\n",
    "f1 = F1Score(num_classes=2)\n",
    "print(f1(preds, target))\n",
    "\n",
    "print(\"============= F1 Score ============\")\n",
    "f1 = F1Score()\n",
    "print(f1(preds, target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch-img-seg': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb640731e92c903421fc8e3a24c104902d308b890b29b3d89d8138718d713a46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
